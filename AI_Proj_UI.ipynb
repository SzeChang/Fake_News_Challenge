{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Proj UI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to use\n",
        "\n",
        "Code for UI itself MUST be encapsulated in a separate .py file (cannot run as cells in ipynb).\n",
        "\n",
        "In this notebook, I use `%% writefile app.py` to write a new streamlit .py file to the local venv (you can check with `!ls`).\n",
        "\n",
        "Once your streamlit .py file is ready, to run, skip forward to the last section: **Running streamlit instance**.\n",
        "\n",
        "# Resources\n",
        "\n",
        "[Main page](https://streamlit.io/)\n",
        "\n",
        "[Documentation](https://docs.streamlit.io/library/get-started/main-concepts)\n",
        "\n",
        "[Youtube tutorial](https://github.com/dataprofessor/ml-app)\n",
        "\n",
        "[Linking streamlit and colab](https://medium.com/@jcharistech/how-to-run-streamlit-apps-from-colab-29b969a1bdfc)"
      ],
      "metadata": {
        "id": "zywRdRNwxT21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run https://raw.githubusercontent.com/streamlit/demo-uber-nyc-pickups/master/streamlit_app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joXkBX4EwO7l",
        "outputId": "88b56505-ee34-4a0b-f97d-359fb9704bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-07-25 09:49:33.408 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.73.210.146:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for streamlit app starts here"
      ],
      "metadata": {
        "id": "ytwFqFLy9kiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "transformers==4.21.0 # for BERT, pytorch already inbuilt to colab\n",
        "streamlit==1.11.1\n",
        "pyngrok==4.1.1 # newer versions don't work"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_IzUBMyW-oR",
        "outputId": "cb46d6c6-3cef-4b20-cfcf-911a4a98d804"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yZ4RHOCRvujl",
        "outputId": "0f6d7208-ba16-43b0-d5e8-b1a21a48889c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.21.0\n",
            "  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 6.9 MB/s \n",
            "\u001b[?25hCollecting streamlit==1.11.1\n",
            "  Downloading streamlit-1.11.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 3.1 MB/s \n",
            "\u001b[?25hCollecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0->-r requirements.txt (line 2)) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0->-r requirements.txt (line 2)) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 21.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0->-r requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0->-r requirements.txt (line 2)) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0->-r requirements.txt (line 2)) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0->-r requirements.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (5.1.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (1.5.1)\n",
            "Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (22.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (3.17.3)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (4.2.0)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-12.5.1-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 42.5 MB/s \n",
            "\u001b[?25hCollecting watchdog\n",
            "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 29.0 MB/s \n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (4.1.1)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 45.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (2.13.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (4.2.4)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit==1.11.1->-r requirements.txt (line 3)) (0.10.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.11.1->-r requirements.txt (line 3)) (2.11.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.11.1->-r requirements.txt (line 3)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.11.1->-r requirements.txt (line 3)) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.11.1->-r requirements.txt (line 3)) (0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.0->-r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.11.1->-r requirements.txt (line 3)) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.11.1->-r requirements.txt (line 3)) (0.18.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.0->-r requirements.txt (line 2)) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit==1.11.1->-r requirements.txt (line 3)) (2022.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit==1.11.1->-r requirements.txt (line 3)) (1.15.0)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.15.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (5.1.1)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (7.7.1)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.34.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 47.0 MB/s \n",
            "\u001b[?25hCollecting jupyter-client>=6.1.12\n",
            "  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (1.5.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (5.4.8)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.1.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (23.2.0)\n",
            "Collecting tornado>=5.0\n",
            "  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n",
            "\u001b[K     |████████████████████████████████| 423 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (2.6.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.30-py3-none-any.whl (381 kB)\n",
            "\u001b[K     |████████████████████████████████| 381 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.18.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (1.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==1.11.1->-r requirements.txt (line 3)) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (4.11.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.2.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0->-r requirements.txt (line 2)) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0->-r requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0->-r requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[K     |████████████████████████████████| 51 kB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.13.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (1.8.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (5.4.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (5.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (2.16.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.11.1->-r requirements.txt (line 3)) (0.5.1)\n",
            "Building wheels for collected packages: pyngrok, validators\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15983 sha256=0b05f5231a947e9992ad8ebdee8e19b841b51c2cc33fac8a80890de71207040d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=5e8923c592c592e82a75aab3755e63754bcb07946e1490cb094dda87e82f27ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built pyngrok validators\n",
            "Installing collected packages: tornado, prompt-toolkit, jupyter-client, ipython, ipykernel, smmap, pyyaml, gitdb, commonmark, watchdog, validators, tokenizers, rich, pympler, pydeck, huggingface-hub, gitpython, blinker, transformers, streamlit, pyngrok\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "nbclient 0.6.6 requires traitlets>=5.2.2, but you have traitlets 5.1.1 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.30 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.15.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.34.0 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\u001b[0m\n",
            "Successfully installed blinker-1.5 commonmark-0.9.1 gitdb-4.0.9 gitpython-3.1.27 huggingface-hub-0.8.1 ipykernel-6.15.1 ipython-7.34.0 jupyter-client-7.3.4 prompt-toolkit-3.0.30 pydeck-0.7.1 pympler-1.0.1 pyngrok-4.1.1 pyyaml-6.0 rich-12.5.1 smmap-5.0.0 streamlit-1.11.1 tokenizers-0.12.1 tornado-6.2 transformers-4.21.0 validators-0.20.0 watchdog-2.1.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit",
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# install dependencies\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use this to check if installation is working\n",
        "!streamlit hello\n",
        "# need to stop this manually before running your own streamlit instance!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6P3o7v1wASq",
        "outputId": "567ff355-2218-4377-ff0f-1648397304b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-04 10:23:03.599 INFO    numexpr.utils: NumExpr defaulting to 2 threads.\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  Welcome to Streamlit. Check out our demo in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.2:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.243.188.1:8501\u001b[0m\n",
            "\u001b[0m\n",
            "  Ready to create your own Python apps super quickly?\u001b[0m\n",
            "  Head over to \u001b[0m\u001b[1mhttps://docs.streamlit.io\u001b[0m\n",
            "\u001b[0m\n",
            "  May you create awesome apps!\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT USE ME\n",
        "# get train/test dataset\n",
        "!wget https://raw.githubusercontent.com/FakeNewsChallenge/fnc-1/master/train_stances.csv\n",
        "!wget https://raw.githubusercontent.com/FakeNewsChallenge/fnc-1/master/train_bodies.csv\n",
        "!wget https://raw.githubusercontent.com/FakeNewsChallenge/fnc-1/master/competition_test_bodies.csv\n",
        "!wget https://raw.githubusercontent.com/FakeNewsChallenge/fnc-1/master/competition_test_stances.csv\n",
        "\n",
        "# get pretrained BERT model\n",
        "!wget https://github.com/SzeChang/Fake_News_Challenge/blob/main/ModelWeight/fake_model_CNN_LSTM.pt"
      ],
      "metadata": {
        "id": "WP4usmNQDA7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check all required files downloaded\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyxNT2f1D3oZ",
        "outputId": "4187a921-8724-43ed-a2e3-aa0588832a9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "competition_test_bodies.csv   fake_model_CNN_LSTM.pt  train_bodies.csv\n",
            "competition_test_stances.csv  sample_data\t      train_stances.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8Hvcl_aq12p",
        "outputId": "15a408ab-ea8e-4a5e-f8d9-ddefa05c78e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 29.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 14.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BERT_Arch(nn.Module):\n",
        "    def __init__(self, bert_head, bert_body):\n",
        "      super(BERT_Arch, self).__init__()\n",
        "      self.bert_head = bert_head\n",
        "      self.bert_body = bert_body\n",
        "      # Max pooling layer \n",
        "      self.max_pooling = nn.MaxPool1d(4,stride=4)\n",
        "      # dropout layer\n",
        "      self.dropout = nn.Dropout(0.1)\n",
        "      # relu activation function\n",
        "      self.relu =  nn.ReLU()\n",
        "      # dense layer 1\n",
        "      self.fc = nn.Linear(384,768)\n",
        "      self.fc1 = nn.Linear(768,512)\n",
        "      # dense layer 2 (Output layer)\n",
        "      self.fc2 = nn.Linear(512,4)\n",
        "      #softmax activation function \n",
        "      self.softmax = nn.LogSoftmax(dim=1)\n",
        " \n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id_head, sent_id_body, mask_head, mask_body):\n",
        "      # print(sent_id.size()) \n",
        "      # print(mask.size()) \n",
        "      #pass the inputs to the model   \n",
        "      _, cls_hs_h = self.bert_head(sent_id_head, attention_mask=mask_head) \n",
        "      _, cls_hs_b = self.bert_body(sent_id_body, attention_mask=mask_body) \n",
        "      cls_hs = torch.cat((cls_hs_h,cls_hs_b),dim=1) \n",
        "      max_pool_out =torch.squeeze(self.max_pooling(cls_hs.unsqueeze(0))) \n",
        "      fc_out = self.fc(max_pool_out)\n",
        "      fc_act_out = self.relu(fc_out)\n",
        "      x = self.fc1(fc_act_out)\n",
        "      x = self.relu(x)\n",
        "      x = self.dropout(x)\n",
        "      # output layer\n",
        "      x = self.fc2(x)\n",
        "      # apply softmax activation\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "5vgPCLrLp5HK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "PATH = 'saved_weights_bert_2.pt'\n",
        "\n",
        "bert_head = AutoModel.from_pretrained('bert-base-uncased')\n",
        "bert_body = AutoModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = BERT_Arch(bert_head, bert_body)\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load(PATH))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "FI8Qgsk8lrgL",
        "outputId": "2882d4d5-1605-47a4-ac57-ab265477fa02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f610fb1338bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERT_Arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    938\u001b[0m         typed_storage._storage._set_from_file(\n\u001b[1;32m    939\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             torch._utils._element_size(typed_storage.dtype))\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: unexpected EOF, expected 1151652 more bytes. The file might be corrupted."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test if can just writefile the model and import into app.py \n",
        "# YES it works!\n",
        "%%writefile danson.py\n",
        "\n",
        "def moonlighter():\n",
        "  return \"danson\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKOzlfqYHhAR",
        "outputId": "ea01b947-7edb-45e5-e936-74131ddf4ff8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting danson.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working file"
      ],
      "metadata": {
        "id": "mbSwlWBJWJgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# streamlit base .py file\n",
        "# demonstrates basic functionality of a streamlit app\n",
        "\n",
        "\n",
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from danson import *\n",
        "\n",
        "#---------------------------------#\n",
        "# Page layout\n",
        "\n",
        "PAGE_CONFIG = {\"page_title\":\"AI Project Group 18\",\n",
        "              \"page_icon\":\":newspaper:\",\n",
        "              \"layout\":\"wide\"}\n",
        "st.set_page_config(**PAGE_CONFIG)\n",
        "\n",
        "#---------------------------------#\n",
        "# Model building\n",
        "\n",
        "\n",
        "\n",
        "#---------------------------------#\n",
        "# Main panel\n",
        "\n",
        "st.write(\"\"\"\n",
        "# The Moonlighter: Danson Lim\n",
        "\n",
        "In this implementation, the *BERT* model is used to train for Fake News Stance Detection... *(TO UPDATE THIS)*\n",
        "\n",
        "Try adjusting the hyperparameters in the sidebar!\n",
        "\"\"\")\n",
        "\n",
        "user_input = st.text_area(\"Enter your news here:\")\n",
        "\n",
        "st.write(\"Below should output Danson.\")\n",
        "st.write(moonlighter())\n",
        "\n",
        "#---------------------------------#\n",
        "# Sidebar - Collects user input features into dataframe\n",
        "\n",
        "st.sidebar.title(\"Model Customisation Tools\")\n",
        "\n",
        "stance_type = st.sidebar.radio(\"Stance Type\", (\"Agree\", \"Disagree\", \"Discuss\", \"Unrelated\"), index=3, key=3)\n",
        "if stance_type == \"Agree\":\n",
        "    st.markdown(\"## Agree\")\n",
        "elif stance_type == \"Disagree\":\n",
        "\t  st.markdown(\"## Disagree\")\n",
        "elif stance_type == \"Discuss\":\n",
        "\t  st.markdown(\"## Discuss\")\n",
        "elif stance_type == \"Unrelated\":\n",
        "\t  st.markdown(\"## Unrelated\")\n",
        "\n",
        "with st.sidebar.header(\"1. Set Parameters\"):\n",
        "    split_size = st.sidebar.slider('Data split ratio (% for Training Set)', 10, 90, 80, 5)\n",
        "\n",
        "with st.sidebar.subheader(\"2. Learning Parameters\"):\n",
        "    parameter_n_estimators = st.sidebar.slider('Number of estimators (n_estimators)', 0, 1000, 100, 100)\n",
        "    parameter_max_features = st.sidebar.select_slider('Max features (max_features)', options=['auto', 'sqrt', 'log2'])\n",
        "    parameter_min_samples_split = st.sidebar.slider('Minimum number of samples required to split an internal node (min_samples_split)', 1, 10, 2, 1)\n",
        "    parameter_min_samples_leaf = st.sidebar.slider('Minimum number of samples required to be at a leaf node (min_samples_leaf)', 1, 10, 2, 1)\n",
        "\n",
        "with st.sidebar.subheader(\"3. General Parameters\"):\n",
        "    parameter_random_state = st.sidebar.slider('Seed number (random_state)', 0, 1000, 42, 1)\n",
        "    parameter_criterion = st.sidebar.select_slider('Performance measure (criterion)', options=['mse', 'mae'])\n",
        "    parameter_bootstrap = st.sidebar.select_slider('Bootstrap samples when building trees (bootstrap)', options=[True, False])\n",
        "    parameter_oob_score = st.sidebar.select_slider('Whether to use out-of-bag samples to estimate the R^2 on unseen data (oob_score)', options=[False, True])\n",
        "    parameter_n_jobs = st.sidebar.select_slider('Number of jobs to run in parallel (n_jobs)', options=[1, -1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koCKLYPv9q-P",
        "outputId": "999fb6ce-03a8-40cb-c84a-98545cf75fbc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML app file (from online)"
      ],
      "metadata": {
        "id": "GwPpdz77WNl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ml-app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import load_diabetes, load_boston\n",
        "\n",
        "#---------------------------------#\n",
        "# Page layout\n",
        "## Page expands to full width\n",
        "st.set_page_config(page_title='The Machine Learning App',\n",
        "    layout='wide')\n",
        "\n",
        "#---------------------------------#\n",
        "# Model building\n",
        "def build_model(df):\n",
        "    X = df.iloc[:,:-1] # Using all column except for the last column as X\n",
        "    Y = df.iloc[:,-1] # Selecting the last column as Y\n",
        "\n",
        "    # Data splitting\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=(100-split_size)/100)\n",
        "    \n",
        "    st.markdown('**1.2. Data splits**')\n",
        "    st.write('Training set')\n",
        "    st.info(X_train.shape)\n",
        "    st.write('Test set')\n",
        "    st.info(X_test.shape)\n",
        "\n",
        "    st.markdown('**1.3. Variable details**:')\n",
        "    st.write('X variable')\n",
        "    st.info(list(X.columns))\n",
        "    st.write('Y variable')\n",
        "    st.info(Y.name)\n",
        "\n",
        "    rf = RandomForestRegressor(n_estimators=parameter_n_estimators,\n",
        "        random_state=parameter_random_state,\n",
        "        max_features=parameter_max_features,\n",
        "        criterion=parameter_criterion,\n",
        "        min_samples_split=parameter_min_samples_split,\n",
        "        min_samples_leaf=parameter_min_samples_leaf,\n",
        "        bootstrap=parameter_bootstrap,\n",
        "        oob_score=parameter_oob_score,\n",
        "        n_jobs=parameter_n_jobs)\n",
        "    rf.fit(X_train, Y_train)\n",
        "\n",
        "    st.subheader('2. Model Performance')\n",
        "\n",
        "    st.markdown('**2.1. Training set**')\n",
        "    Y_pred_train = rf.predict(X_train)\n",
        "    st.write('Coefficient of determination ($R^2$):')\n",
        "    st.info( r2_score(Y_train, Y_pred_train) )\n",
        "\n",
        "    st.write('Error (MSE or MAE):')\n",
        "    st.info( mean_squared_error(Y_train, Y_pred_train) )\n",
        "\n",
        "    st.markdown('**2.2. Test set**')\n",
        "    Y_pred_test = rf.predict(X_test)\n",
        "    st.write('Coefficient of determination ($R^2$):')\n",
        "    st.info( r2_score(Y_test, Y_pred_test) )\n",
        "\n",
        "    st.write('Error (MSE or MAE):')\n",
        "    st.info( mean_squared_error(Y_test, Y_pred_test) )\n",
        "\n",
        "    st.subheader('3. Model Parameters')\n",
        "    st.write(rf.get_params())\n",
        "\n",
        "#---------------------------------#\n",
        "st.write(\"\"\"\n",
        "\n",
        "# The Machine Learning App\n",
        "\n",
        "In this implementation, the *RandomForestRegressor()* function is used in this app for build a regression model using the **Random Forest** algorithm.\n",
        "\n",
        "Try adjusting the hyperparameters!\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "#---------------------------------#\n",
        "# Sidebar - Collects user input features into dataframe\n",
        "with st.sidebar.header('1. Upload your CSV data'):\n",
        "    uploaded_file = st.sidebar.file_uploader(\"Upload your input CSV file\", type=[\"csv\"])\n",
        "    st.sidebar.markdown(\"\"\"\n",
        "[Example CSV input file](https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv)\n",
        "\"\"\")\n",
        "\n",
        "# Sidebar - Specify parameter settings\n",
        "with st.sidebar.header('2. Set Parameters'):\n",
        "    split_size = st.sidebar.slider('Data split ratio (% for Training Set)', 10, 90, 80, 5)\n",
        "\n",
        "with st.sidebar.subheader('2.1. Learning Parameters'):\n",
        "    parameter_n_estimators = st.sidebar.slider('Number of estimators (n_estimators)', 0, 1000, 100, 100)\n",
        "    parameter_max_features = st.sidebar.select_slider('Max features (max_features)', options=['auto', 'sqrt', 'log2'])\n",
        "    parameter_min_samples_split = st.sidebar.slider('Minimum number of samples required to split an internal node (min_samples_split)', 1, 10, 2, 1)\n",
        "    parameter_min_samples_leaf = st.sidebar.slider('Minimum number of samples required to be at a leaf node (min_samples_leaf)', 1, 10, 2, 1)\n",
        "\n",
        "with st.sidebar.subheader('2.2. General Parameters'):\n",
        "    parameter_random_state = st.sidebar.slider('Seed number (random_state)', 0, 1000, 42, 1)\n",
        "    parameter_criterion = st.sidebar.select_slider('Performance measure (criterion)', options=['mse', 'mae'])\n",
        "    parameter_bootstrap = st.sidebar.select_slider('Bootstrap samples when building trees (bootstrap)', options=[True, False])\n",
        "    parameter_oob_score = st.sidebar.select_slider('Whether to use out-of-bag samples to estimate the R^2 on unseen data (oob_score)', options=[False, True])\n",
        "    parameter_n_jobs = st.sidebar.select_slider('Number of jobs to run in parallel (n_jobs)', options=[1, -1])\n",
        "\n",
        "#---------------------------------#\n",
        "# Main panel\n",
        "\n",
        "# Displays the dataset\n",
        "st.subheader('1. Dataset')\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    df = pd.read_csv(uploaded_file)\n",
        "    st.markdown('**1.1. Glimpse of dataset**')\n",
        "    st.write(df)\n",
        "    build_model(df)\n",
        "else:\n",
        "    st.info('Awaiting for CSV file to be uploaded.')\n",
        "    if st.button('Press to use Example Dataset'):\n",
        "        # Boston housing dataset\n",
        "        boston = load_boston()\n",
        "        X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "        Y = pd.Series(boston.target, name='response')\n",
        "        df = pd.concat( [X,Y], axis=1 )\n",
        "\n",
        "        st.markdown('The Boston housing dataset is used as the example.')\n",
        "        st.write(df.head(5))\n",
        "\n",
        "        build_model(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BIOGRaxS5x3",
        "outputId": "3e542e16-96d1-47ca-f2d8-ece1f679d147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ml-app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GitHub file (penguin)"
      ],
      "metadata": {
        "id": "K9ZU8F7OWTOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dataprofessor/code.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDOgZRncES0j",
        "outputId": "bc732031-4a9d-451b-d555-c6549f5a0320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'code'...\n",
            "remote: Enumerating objects: 657, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/21)\u001b[K\rremote: Counting objects:   9% (2/21)\u001b[K\rremote: Counting objects:  14% (3/21)\u001b[K\rremote: Counting objects:  19% (4/21)\u001b[K\rremote: Counting objects:  23% (5/21)\u001b[K\rremote: Counting objects:  28% (6/21)\u001b[K\rremote: Counting objects:  33% (7/21)\u001b[K\rremote: Counting objects:  38% (8/21)\u001b[K\rremote: Counting objects:  42% (9/21)\u001b[K\rremote: Counting objects:  47% (10/21)\u001b[K\rremote: Counting objects:  52% (11/21)\u001b[K\rremote: Counting objects:  57% (12/21)\u001b[K\rremote: Counting objects:  61% (13/21)\u001b[K\rremote: Counting objects:  66% (14/21)\u001b[K\rremote: Counting objects:  71% (15/21)\u001b[K\rremote: Counting objects:  76% (16/21)\u001b[K\rremote: Counting objects:  80% (17/21)\u001b[K\rremote: Counting objects:  85% (18/21)\u001b[K\rremote: Counting objects:  90% (19/21)\u001b[K\rremote: Counting objects:  95% (20/21)\u001b[K\rremote: Counting objects: 100% (21/21)\u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 657 (delta 7), reused 15 (delta 3), pack-reused 636\u001b[K\n",
            "Receiving objects: 100% (657/657), 31.78 MiB | 25.83 MiB/s, done.\n",
            "Resolving deltas: 100% (296/296), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd code/streamlit/part3\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDHxL7nGFEQV",
        "outputId": "47613b09-5b45-4d8f-c624-0854325fba8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/code/streamlit/part3\n",
            "penguins-app.py       penguins_clf.pkl\t    penguins-model-building.py\n",
            "penguins_cleaned.csv  penguins_example.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test penguin-app.py file, from https://www.youtube.com/watch?v=Eai1jaZrRDs&list=PLtqF5YXg7GLmCvTswG32NqQypOuYkPRUE&index=3\n",
        "%%writefile penguin-app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "st.write(\"\"\"\n",
        "# Penguin Prediction App\n",
        "This app predicts the **Palmer Penguin** species!\n",
        "Data obtained from the [palmerpenguins library](https://github.com/allisonhorst/palmerpenguins) in R by Allison Horst.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.header('User Input Features')\n",
        "\n",
        "st.sidebar.markdown(\"\"\"\n",
        "[Example CSV input file](https://raw.githubusercontent.com/dataprofessor/data/master/penguins_example.csv)\n",
        "\"\"\")\n",
        "\n",
        "# Collects user input features into dataframe\n",
        "uploaded_file = st.sidebar.file_uploader(\"Upload your input CSV file\", type=[\"csv\"])\n",
        "if uploaded_file is not None:\n",
        "    input_df = pd.read_csv(uploaded_file)\n",
        "else:\n",
        "    def user_input_features():\n",
        "        island = st.sidebar.selectbox('Island',('Biscoe','Dream','Torgersen'))\n",
        "        sex = st.sidebar.selectbox('Sex',('male','female'))\n",
        "        bill_length_mm = st.sidebar.slider('Bill length (mm)', 32.1,59.6,43.9)\n",
        "        bill_depth_mm = st.sidebar.slider('Bill depth (mm)', 13.1,21.5,17.2)\n",
        "        flipper_length_mm = st.sidebar.slider('Flipper length (mm)', 172.0,231.0,201.0)\n",
        "        body_mass_g = st.sidebar.slider('Body mass (g)', 2700.0,6300.0,4207.0)\n",
        "        data = {'island': island,\n",
        "                'bill_length_mm': bill_length_mm,\n",
        "                'bill_depth_mm': bill_depth_mm,\n",
        "                'flipper_length_mm': flipper_length_mm,\n",
        "                'body_mass_g': body_mass_g,\n",
        "                'sex': sex}\n",
        "        features = pd.DataFrame(data, index=[0])\n",
        "        return features\n",
        "    input_df = user_input_features()\n",
        "\n",
        "# Combines user input features with entire penguins dataset\n",
        "# This will be useful for the encoding phase\n",
        "penguins_url = 'https://raw.githubusercontent.com/dataprofessor/code/master/streamlit/part3/penguins_cleaned.csv'\n",
        "penguins_raw = pd.read_csv(penguins_url)\n",
        "penguins = penguins_raw.drop(columns=['species'])\n",
        "df = pd.concat([input_df,penguins],axis=0)\n",
        "\n",
        "# Encoding of ordinal features\n",
        "# https://www.kaggle.com/pratik1120/penguin-dataset-eda-classification-and-clustering\n",
        "encode = ['sex','island']\n",
        "for col in encode:\n",
        "    dummy = pd.get_dummies(df[col], prefix=col)\n",
        "    df = pd.concat([df,dummy], axis=1)\n",
        "    del df[col]\n",
        "df = df[:1] # Selects only the first row (the user input data)\n",
        "\n",
        "# Displays the user input features\n",
        "st.subheader('User Input features')\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    st.write(df)\n",
        "else:\n",
        "    st.write('Awaiting CSV file to be uploaded. Currently using example input parameters (shown below).')\n",
        "    st.write(df)\n",
        "\n",
        "# Reads in saved classification model (need to reupload when runtime is restarted)\n",
        "penguins_pickle = open(DATA_PATH+'/penguins_clf.pkl','rb')\n",
        "load_clf = pickle.load(penguins_pickle)\n",
        "\n",
        "# Apply model to make predictions\n",
        "prediction = load_clf.predict(df)\n",
        "prediction_proba = load_clf.predict_proba(df)\n",
        "\n",
        "\n",
        "st.subheader('Prediction')\n",
        "penguins_species = np.array(['Adelie','Chinstrap','Gentoo'])\n",
        "st.write(penguins_species[prediction])\n",
        "\n",
        "st.subheader('Prediction Probability')\n",
        "st.write(prediction_proba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URHNs1xJ7hx4",
        "outputId": "76675a95-fd4b-43c8-bfb5-6cf3acb49c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting penguin-app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running streamlit instance"
      ],
      "metadata": {
        "id": "hdVnHcy0WYfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check if app.py has been written to colab sandbox\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQgdn4me9umj",
        "outputId": "2c70a678-f326-435b-d73e-412c44ac1184"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tdanson.py  requirements.txt  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ngrok authentication, only needs to be done once at start of runtime\n",
        "!ngrok authtoken 2CQtJERhcUlxLR6cdKdzfP8J9jC_56J8CecbbnGjX8dp1tE4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5hZtYwGCndT",
        "outputId": "f4222bdc-5154-4bfe-d72e-7e4be3fdea14"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start streamlit app instance\n",
        "!streamlit run app.py &>/dev/null& # change app.py to your streamlit app name\n",
        "!pgrep streamlit # outputs streamlit process number (required for killing)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAvCyxRoB0h2",
        "outputId": "61d280fe-1bfb-4fad-e52b-d3730a083c71"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "# setup tunnel to 8501 (streamlit port)\n",
        "pub_url = ngrok.connect(port='8501')\n",
        "print(pub_url) # generates url for app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7s6SW8BZBUj9",
        "outputId": "f49e6549-4126-4862-cb79-f8f7cd0e50bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://46a5-34-125-122-152.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shutdown\n",
        "!kill 288 # change the process number\n",
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "X3CZo-rhDMzp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}